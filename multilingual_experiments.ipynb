{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d86ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: seqeval in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from seqeval) (1.7.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from requests->transformers) (2.6.1)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets seqeval sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8622119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIO labels count: 19\n",
      "Example labels: ['O', 'B-AADHAAR', 'B-PAN', 'B-PHONE', 'B-EMAIL', 'B-PIN', 'B-PERSON', 'B-ORG']\n"
     ]
    }
   ],
   "source": [
    "# cell 2: Labels (BIO)\n",
    "BASE_LABELS = [\"AADHAAR\",\"PAN\",\"PHONE\",\"EMAIL\",\"PIN\",\"PERSON\",\"ORG\",\"LOC\",\"DATE\"]\n",
    "# build BIO label set\n",
    "BIO_LABELS = [\"O\"] + [f\"B-{l}\" for l in BASE_LABELS] + [f\"I-{l}\" for l in BASE_LABELS]\n",
    "label2id = {l:i for i,l in enumerate(BIO_LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "print(\"BIO labels count:\", len(BIO_LABELS))\n",
    "print(\"Example labels:\", BIO_LABELS[:8])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c748be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to public model: xlm-roberta-base \n",
      "Reason: name 'AutoTokenizer' is not defined\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xlm-roberta-base'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 3\n",
    "# Preferred model (may be gated) and a public fallback. Use fallback to avoid 401 errors.\n",
    "PREFERRED = \"ai4bharat/indic-bert\"   # try later if you have access/token\n",
    "FALLBACK = \"xlm-roberta-base\"        # public, works for the pipeline test\n",
    "\n",
    "# Try to load preferred; if not accessible, fallback automatically.\n",
    "model_name = None\n",
    "try:\n",
    "    # Try preferred without token first â€” will fail on gated repos\n",
    "    AutoTokenizer.from_pretrained(PREFERRED)\n",
    "    model_name = PREFERRED\n",
    "    print(\"Using preferred model:\", PREFERRED)\n",
    "except Exception as e:\n",
    "    model_name = FALLBACK\n",
    "    print(\"Falling back to public model:\", FALLBACK, \"\\nReason:\", str(e))\n",
    "\n",
    "# set model_name variable used downstream\n",
    "model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678a35de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'words': ['To,',\n",
       "   'The',\n",
       "   'Public',\n",
       "   'Information',\n",
       "   'Officer,',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Transport,',\n",
       "   'Uttar',\n",
       "   'Pradesh.',\n",
       "   'Subject:',\n",
       "   'Information',\n",
       "   'under',\n",
       "   'RTI',\n",
       "   'Act,',\n",
       "   '2005',\n",
       "   'regarding',\n",
       "   'driving',\n",
       "   'licence',\n",
       "   'issuance.',\n",
       "   'Sir/Madam,',\n",
       "   'Please',\n",
       "   'provide',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'driving',\n",
       "   'licences',\n",
       "   'issued',\n",
       "   'in',\n",
       "   'Lucknow',\n",
       "   'district',\n",
       "   'between',\n",
       "   'January',\n",
       "   'and',\n",
       "   'March',\n",
       "   '2024.',\n",
       "   'Also',\n",
       "   'specify',\n",
       "   'the',\n",
       "   'average',\n",
       "   'processing',\n",
       "   'time',\n",
       "   'and',\n",
       "   'whether',\n",
       "   'the',\n",
       "   'department',\n",
       "   'has',\n",
       "   'any',\n",
       "   'online',\n",
       "   'grievance',\n",
       "   'redressal',\n",
       "   'mechanism.',\n",
       "   'Applicant:',\n",
       "   'Ritika',\n",
       "   'Singh',\n",
       "   'S/o',\n",
       "   'Ramesh',\n",
       "   'Kumar',\n",
       "   'Singh',\n",
       "   'R/o',\n",
       "   'C-45,',\n",
       "   'Gomti',\n",
       "   'Nagar,',\n",
       "   'Lucknow',\n",
       "   '-',\n",
       "   '226010',\n",
       "   'Phone:',\n",
       "   '9936521874',\n",
       "   'Email:',\n",
       "   'ritika.singh25@gmail.com',\n",
       "   'Aadhaar:',\n",
       "   '4589',\n",
       "   '1234',\n",
       "   '7789'],\n",
       "  'labels': ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-PERSON',\n",
       "   'I-PERSON',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-AADHAAR',\n",
       "   'I-AADHAAR',\n",
       "   'I-AADHAAR',\n",
       "   'I-AADHAAR',\n",
       "   'I-AADHAAR',\n",
       "   'I-AADHAAR',\n",
       "   'I-AADHAAR',\n",
       "   'I-AADHAAR',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  'fname': 'sample1.txt',\n",
       "  'text': 'To,\\nThe Public Information Officer,\\nDepartment of Transport,\\nUttar Pradesh.\\n\\nSubject: Information under RTI Act, 2005 regarding driving licence issuance.\\n\\nSir/Madam,\\nPlease provide the number of driving licences issued in Lucknow district between January and March 2024. Also specify the average processing time and whether the department has any online grievance redressal mechanism.\\n\\nApplicant:\\nRitika Singh\\nS/o Ramesh Kumar Singh\\nR/o C-45, Gomti Nagar, Lucknow - 226010\\nPhone: 9936521874\\nEmail: ritika.singh25@gmail.com\\nAadhaar: 4589 1234 7789\\n',\n",
       "  'word_idxs': [(0, 3),\n",
       "   (4, 7),\n",
       "   (8, 14),\n",
       "   (15, 26),\n",
       "   (27, 35),\n",
       "   (36, 46),\n",
       "   (47, 49),\n",
       "   (50, 60),\n",
       "   (61, 66),\n",
       "   (67, 75),\n",
       "   (77, 85),\n",
       "   (86, 97),\n",
       "   (98, 103),\n",
       "   (104, 107),\n",
       "   (108, 112),\n",
       "   (113, 117),\n",
       "   (118, 127),\n",
       "   (128, 135),\n",
       "   (136, 143),\n",
       "   (144, 153),\n",
       "   (155, 165),\n",
       "   (166, 172),\n",
       "   (173, 180),\n",
       "   (181, 184),\n",
       "   (185, 191),\n",
       "   (192, 194),\n",
       "   (195, 202),\n",
       "   (203, 211),\n",
       "   (212, 218),\n",
       "   (219, 221),\n",
       "   (222, 229),\n",
       "   (230, 238),\n",
       "   (239, 246),\n",
       "   (247, 254),\n",
       "   (255, 258),\n",
       "   (259, 264),\n",
       "   (265, 270),\n",
       "   (271, 275),\n",
       "   (276, 283),\n",
       "   (284, 287),\n",
       "   (288, 295),\n",
       "   (296, 306),\n",
       "   (307, 311),\n",
       "   (312, 315),\n",
       "   (316, 323),\n",
       "   (324, 327),\n",
       "   (328, 338),\n",
       "   (339, 342),\n",
       "   (343, 346),\n",
       "   (347, 353),\n",
       "   (354, 363),\n",
       "   (364, 373),\n",
       "   (374, 384),\n",
       "   (386, 396),\n",
       "   (397, 403),\n",
       "   (404, 409),\n",
       "   (410, 413),\n",
       "   (414, 420),\n",
       "   (421, 426),\n",
       "   (427, 432),\n",
       "   (433, 436),\n",
       "   (437, 442),\n",
       "   (443, 448),\n",
       "   (449, 455),\n",
       "   (456, 463),\n",
       "   (464, 465),\n",
       "   (466, 472),\n",
       "   (473, 479),\n",
       "   (480, 490),\n",
       "   (491, 497),\n",
       "   (498, 522),\n",
       "   (523, 531),\n",
       "   (532, 536),\n",
       "   (537, 541),\n",
       "   (542, 546)]}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 4: load gold -> word tokens + BIO labels\n",
    "import os, json, re\n",
    "\n",
    "GOLD_PATH = \"gold.json\"\n",
    "RTIS_DIR = \"rtis\"\n",
    "\n",
    "def compute_word_spans(text, words):\n",
    "    # compute start/end char index for each word occurrence using a cursor\n",
    "    idxs = []\n",
    "    cur = 0\n",
    "    for w in words:\n",
    "        start = text.find(w, cur)\n",
    "        if start == -1:\n",
    "            # fallback: advance cursor until we find plausible match (rare)\n",
    "            start = cur\n",
    "        idxs.append((start, start + len(w)))\n",
    "        cur = start + len(w)\n",
    "    return idxs\n",
    "\n",
    "def span_label_tokens(word_idxs, spans):\n",
    "    # spans: list of dicts with start,end,label\n",
    "    labels = [\"O\"] * len(word_idxs)\n",
    "    for s in spans:\n",
    "        s0, s1, lab = s[\"start\"], s[\"end\"], s[\"label\"]\n",
    "        # clamp label name mapping\n",
    "        lab = lab.upper()\n",
    "        for i,(ws,we) in enumerate(word_idxs):\n",
    "            # if word intersects span\n",
    "            if not (we <= s0 or ws >= s1):\n",
    "                labels[i] = lab\n",
    "    # convert to BIO: contiguous tokens of same label -> B- / I-\n",
    "    bio = []\n",
    "    prev = \"O\"\n",
    "    for lab in labels:\n",
    "        if lab == \"O\":\n",
    "            bio.append(\"O\")\n",
    "            prev = \"O\"\n",
    "        else:\n",
    "            if prev != lab:\n",
    "                bio.append(f\"B-{lab}\")\n",
    "            else:\n",
    "                bio.append(f\"I-{lab}\")\n",
    "            prev = lab\n",
    "    return bio\n",
    "\n",
    "def load_gold_as_examples(gold_path=GOLD_PATH, rtis_folder=RTIS_DIR):\n",
    "    gold = json.load(open(gold_path, encoding='utf-8'))\n",
    "    examples = []\n",
    "    for fname, spans in gold.items():\n",
    "        fpath = os.path.join(rtis_folder, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            print(\"Warning: missing\", fpath)\n",
    "            continue\n",
    "        txt = open(fpath, encoding='utf-8').read()\n",
    "        # simple whitespace split into words (keeps punctuation attached to words)\n",
    "        words = txt.split()\n",
    "        word_idxs = compute_word_spans(txt, words)\n",
    "        labels = span_label_tokens(word_idxs, spans)\n",
    "        examples.append({\"words\": words, \"labels\": labels, \"fname\": fname, \"text\": txt, \"word_idxs\": word_idxs})\n",
    "    return examples\n",
    "\n",
    "examples = load_gold_as_examples()\n",
    "print(f\"Loaded {len(examples)} examples\")\n",
    "examples[:1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39ab7742",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'fname'],\n",
      "        num_rows: 40\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'fname'],\n",
      "        num_rows: 10\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# cell 5: tokenizer + align labels to tokens\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "def tokenize_and_align_words(example, max_length=512):\n",
    "    # example: {\"words\": [...], \"labels\": [...], ...}\n",
    "    tokenized = tokenizer(example[\"words\"], is_split_into_words=True,\n",
    "                          truncation=True, padding=\"max_length\", max_length=max_length, return_tensors=None)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            # label for the first token of the word\n",
    "            lab = example[\"labels\"][word_idx]\n",
    "            aligned_labels.append(label2id.get(lab, label2id[\"O\"]))\n",
    "        else:\n",
    "            # subsequent subtokens\n",
    "            aligned_labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    tokenized[\"fname\"] = example[\"fname\"]\n",
    "    return tokenized\n",
    "\n",
    "# Build dataset list (tokenized)\n",
    "tokenized_list = [tokenize_and_align_words(ex) for ex in examples]\n",
    "from datasets import Dataset, DatasetDict\n",
    "dataset = Dataset.from_list(tokenized_list)\n",
    "if len(dataset) > 5:\n",
    "    dset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    dataset = DatasetDict({\"train\": dset[\"train\"], \"test\": dset[\"test\"]})\n",
    "else:\n",
    "    dataset = DatasetDict({\"train\": dataset, \"test\": dataset})\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00b2db46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ready: seqeval\n",
      "torch: 2.9.1+cpu cuda available: False\n",
      "Loading model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to cpu\n"
     ]
    }
   ],
   "source": [
    "# cell 6: metric & model loading\n",
    "from evaluate import load as load_metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for p, l in zip(preds, label_ids):\n",
    "        pl = []\n",
    "        tl = []\n",
    "        for pred_i, lab_i in zip(p, l):\n",
    "            if lab_i != -100:\n",
    "                pl.append(id2label[pred_i])\n",
    "                tl.append(id2label[lab_i])\n",
    "        true_preds.append(pl)\n",
    "        true_labels.append(tl)\n",
    "    return true_preds, true_labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, label_ids = eval_pred\n",
    "    preds_list, labels_list = align_predictions(logits, label_ids)\n",
    "    # seqeval expects BIO labels like B-PERSON etc.\n",
    "    results = metric.compute(predictions=preds_list, references=labels_list)\n",
    "    overall = {\n",
    "        \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall\": results.get(\"overall_recall\", 0.0),\n",
    "        \"f1\": results.get(\"overall_f1\", 0.0)\n",
    "    }\n",
    "    return overall\n",
    "\n",
    "print(\"Metric ready: seqeval\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "# load model (if already loaded in session skip)\n",
    "try:\n",
    "    model\n",
    "    print(\"model already defined\")\n",
    "except NameError:\n",
    "    print(\"Loading model:\", model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(BIO_LABELS),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(\"Model loaded and moved to\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f84390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aasth\\AppData\\Local\\Temp\\ipykernel_36216\\2186735773.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer args ok. dataset sizes -> train: 40  eval: 10\n",
      "Model device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/30 : < :, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cell 7: training arguments + trainer\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"rti_model_run\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,          # increased\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\"   # depends on compute_metrics returning f1; Trainer expects that key\n",
    ")\n",
    "\n",
    "print(\"Trainer args ok. dataset sizes -> train:\", len(dataset[\"train\"]), \" eval:\", len(dataset[\"test\"]))\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ae57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer args ok. dataset sizes -> train: 3  eval: 3\n",
      "Model device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aasth\\AppData\\Local\\Temp\\ipykernel_20944\\2391809890.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 03:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.894982</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PERSON seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PHONE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: EMAIL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PAN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=2.1222705841064453, metrics={'train_runtime': 452.4232, 'train_samples_per_second': 0.007, 'train_steps_per_second': 0.002, 'total_flos': 783946967040.0, 'train_loss': 2.1222705841064453, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 7 (compat for transformers 4.57.1)\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "\n",
    "# model was loaded earlier as `model`\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Use the argument names that match your transformers version (eval_strategy, save_strategy, logging_strategy)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"rti_model_run\",\n",
    "    eval_strategy=\"epoch\",        # older name in this transformers version\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "print(\"Trainer args ok. dataset sizes -> train:\", len(dataset[\"train\"]), \" eval:\", len(dataset[\"test\"]))\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36da9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to trained_rti_model\n"
     ]
    }
   ],
   "source": [
    "# cell 8\n",
    "OUTDIR = \"trained_rti_model\"\n",
    "trainer.save_model(OUTDIR)\n",
    "tokenizer.save_pretrained(OUTDIR)\n",
    "print(\"Saved model to\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af51081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preds_finetuned.json with entries for 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample1.txt': [{'start': 0, 'end': 46, 'label': 'DATE'},\n",
       "  {'start': 50, 'end': 60, 'label': 'DATE'},\n",
       "  {'start': 67, 'end': 75, 'label': 'DATE'},\n",
       "  {'start': 84, 'end': 107, 'label': 'DATE'},\n",
       "  {'start': 111, 'end': 143, 'label': 'DATE'},\n",
       "  {'start': 148, 'end': 165, 'label': 'DATE'},\n",
       "  {'start': 173, 'end': 211, 'label': 'DATE'},\n",
       "  {'start': 217, 'end': 218, 'label': 'DATE'},\n",
       "  {'start': 225, 'end': 384, 'label': 'DATE'},\n",
       "  {'start': 389, 'end': 396, 'label': 'DATE'},\n",
       "  {'start': 399, 'end': 432, 'label': 'DATE'},\n",
       "  {'start': 434, 'end': 445, 'label': 'DATE'},\n",
       "  {'start': 446, 'end': 455, 'label': 'DATE'},\n",
       "  {'start': 459, 'end': 546, 'label': 'DATE'}],\n",
       " 'sample2.txt': [{'start': 0, 'end': 46, 'label': 'DATE'},\n",
       "  {'start': 50, 'end': 79, 'label': 'DATE'},\n",
       "  {'start': 83, 'end': 89, 'label': 'DATE'},\n",
       "  {'start': 98, 'end': 99, 'label': 'DATE'},\n",
       "  {'start': 102, 'end': 139, 'label': 'DATE'},\n",
       "  {'start': 148, 'end': 167, 'label': 'DATE'},\n",
       "  {'start': 178, 'end': 185, 'label': 'DATE'},\n",
       "  {'start': 189, 'end': 209, 'label': 'DATE'},\n",
       "  {'start': 214, 'end': 294, 'label': 'DATE'},\n",
       "  {'start': 302, 'end': 402, 'label': 'DATE'},\n",
       "  {'start': 404, 'end': 550, 'label': 'DATE'}],\n",
       " 'sample3.txt': [{'start': 0, 'end': 71, 'label': 'DATE'},\n",
       "  {'start': 76, 'end': 77, 'label': 'DATE'},\n",
       "  {'start': 86, 'end': 87, 'label': 'DATE'},\n",
       "  {'start': 88, 'end': 164, 'label': 'DATE'},\n",
       "  {'start': 167, 'end': 238, 'label': 'DATE'},\n",
       "  {'start': 242, 'end': 269, 'label': 'DATE'},\n",
       "  {'start': 282, 'end': 381, 'label': 'DATE'},\n",
       "  {'start': 388, 'end': 414, 'label': 'DATE'},\n",
       "  {'start': 417, 'end': 460, 'label': 'DATE'},\n",
       "  {'start': 462, 'end': 517, 'label': 'DATE'},\n",
       "  {'start': 522, 'end': 535, 'label': 'DATE'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hf inference to preds JSON (run after saving model)\n",
    "from transformers import pipeline\n",
    "import unicodedata\n",
    "from pathlib import Path\n",
    "import json, torch\n",
    "\n",
    "MODEL_DIR = \"trained_rti_model\"\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "nlp_pipe = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=MODEL_DIR,\n",
    "    tokenizer=MODEL_DIR,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "def normalize_text(s):\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = s.replace(\"\\u200c\",\"\").replace(\"\\u200d\",\"\").replace(\"\\ufeff\",\"\")\n",
    "    s = s.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
    "    return s\n",
    "\n",
    "preds = {}\n",
    "for p in sorted(Path(\"rtis\").glob(\"*.txt\")):\n",
    "    raw = p.read_text(encoding=\"utf-8\", errors=\"replace\")\n",
    "    text = normalize_text(raw)\n",
    "    ents = nlp_pipe(text)\n",
    "    spans = []\n",
    "    for e in ents:\n",
    "        lab = e.get(\"entity_group\", e.get(\"label\"))\n",
    "        # normalize label to base label names\n",
    "        if lab is None:\n",
    "            continue\n",
    "        if lab.startswith(\"B-\") or lab.startswith(\"I-\"):\n",
    "            lab = lab.split(\"-\",1)[1]\n",
    "        lab = lab.upper()\n",
    "        # map LOC/GPE to ADDRESS if needed\n",
    "        if lab in (\"LOC\",\"GPE\"):\n",
    "            lab = \"ADDRESS\"\n",
    "        spans.append({\"start\": int(e[\"start\"]), \"end\": int(e[\"end\"]), \"label\": lab, \"text\": text[int(e[\"start\"]):int(e[\"end\"])]})\n",
    "    preds[p.name] = spans\n",
    "\n",
    "Path(\"preds_from_xlmr.json\").write_text(json.dumps(preds, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote preds_from_xlmr.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
