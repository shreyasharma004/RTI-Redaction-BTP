{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d86ab41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (4.4.1)\n",
      "Requirement already satisfied: seqeval in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: fsspec[http]<=2025.10.0,>=2023.1.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (2025.10.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from seqeval) (1.7.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n",
      "Requirement already satisfied: idna in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\aasth\\rti-redaction-btp\\venv\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets seqeval sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8622119b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['O', 'AADHAAR', 'PAN', 'PHONE', 'EMAIL', 'PIN', 'PERSON', 'ORG', 'LOC', 'DATE']\n"
     ]
    }
   ],
   "source": [
    "# cell 2\n",
    "import os, json, numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification,\n",
    "    pipeline\n",
    ")\n",
    "from evaluate import load as load_metric   # from the 'evaluate' package\n",
    "# Labels used in our project. Keep consistent with eval_script / gold.json\n",
    "LABELS = [\"O\",\"AADHAAR\",\"PAN\",\"PHONE\",\"EMAIL\",\"PIN\",\"PERSON\",\"ORG\",\"LOC\",\"DATE\"]\n",
    "label2id = {l:i for i,l in enumerate(LABELS)}\n",
    "id2label = {i:l for l,i in label2id.items()}\n",
    "\n",
    "print(\"Labels:\", LABELS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c748be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to public model: xlm-roberta-base \n",
      "Reason: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/ai4bharat/indic-bert.\n",
      "401 Client Error. (Request ID: Root=1-6914e5f0-4ef1b0103c47c72d62711387;f5745aa3-c4b9-4462-ae0a-3a7fd61147f6)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/ai4bharat/indic-bert/resolve/main/config.json.\n",
      "Access to model ai4bharat/indic-bert is restricted. You must have access to it and be authenticated to access it. Please log in.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'xlm-roberta-base'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 3\n",
    "# Preferred model (may be gated) and a public fallback. Use fallback to avoid 401 errors.\n",
    "PREFERRED = \"ai4bharat/indic-bert\"   # try later if you have access/token\n",
    "FALLBACK = \"xlm-roberta-base\"        # public, works for the pipeline test\n",
    "\n",
    "# Try to load preferred; if not accessible, fallback automatically.\n",
    "model_name = None\n",
    "try:\n",
    "    # Try preferred without token first — will fail on gated repos\n",
    "    AutoTokenizer.from_pretrained(PREFERRED)\n",
    "    model_name = PREFERRED\n",
    "    print(\"Using preferred model:\", PREFERRED)\n",
    "except Exception as e:\n",
    "    model_name = FALLBACK\n",
    "    print(\"Falling back to public model:\", FALLBACK, \"\\nReason:\", str(e))\n",
    "\n",
    "# set model_name variable used downstream\n",
    "model_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2801b63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 examples\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'words': ['To,',\n",
       "   'The',\n",
       "   'Public',\n",
       "   'Information',\n",
       "   'Officer,',\n",
       "   'Department',\n",
       "   'of',\n",
       "   'Transport,',\n",
       "   'Uttar',\n",
       "   'Pradesh.',\n",
       "   'Subject:',\n",
       "   'Information',\n",
       "   'under',\n",
       "   'RTI',\n",
       "   'Act,',\n",
       "   '2005',\n",
       "   'regarding',\n",
       "   'driving',\n",
       "   'licence',\n",
       "   'issuance.',\n",
       "   'Sir/Madam,',\n",
       "   'Please',\n",
       "   'provide',\n",
       "   'the',\n",
       "   'number',\n",
       "   'of',\n",
       "   'driving',\n",
       "   'licences',\n",
       "   'issued',\n",
       "   'in',\n",
       "   'Lucknow',\n",
       "   'district',\n",
       "   'between',\n",
       "   'January',\n",
       "   'and',\n",
       "   'March',\n",
       "   '2024.',\n",
       "   'Also',\n",
       "   'specify',\n",
       "   'the',\n",
       "   'average',\n",
       "   'processing',\n",
       "   'time',\n",
       "   'and',\n",
       "   'whether',\n",
       "   'the',\n",
       "   'department',\n",
       "   'has',\n",
       "   'any',\n",
       "   'online',\n",
       "   'grievance',\n",
       "   'redressal',\n",
       "   'mechanism.',\n",
       "   'Applicant:',\n",
       "   'Ritika',\n",
       "   'Singh',\n",
       "   'S/o',\n",
       "   'Ramesh',\n",
       "   'Kumar',\n",
       "   'Singh',\n",
       "   'R/o',\n",
       "   'C-45,',\n",
       "   'Gomti',\n",
       "   'Nagar,',\n",
       "   'Lucknow',\n",
       "   '–',\n",
       "   '226010',\n",
       "   'Phone:',\n",
       "   '9936521874',\n",
       "   'Email:',\n",
       "   'ritika.singh25@gmail.com',\n",
       "   'Aadhaar:',\n",
       "   '4589',\n",
       "   '1234',\n",
       "   '7789'],\n",
       "  'labels': ['O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'PERSON',\n",
       "   'PERSON',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'PHONE',\n",
       "   'O',\n",
       "   'EMAIL',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  'fname': 'sample1.txt',\n",
       "  'text': 'To,\\nThe Public Information Officer,\\nDepartment of Transport,\\nUttar Pradesh.\\n\\nSubject: Information under RTI Act, 2005 regarding driving licence issuance.\\n\\nSir/Madam,\\nPlease provide the number of driving licences issued in Lucknow district between January and March 2024. Also specify the average processing time and whether the department has any online grievance redressal mechanism.\\n\\nApplicant:\\nRitika Singh\\nS/o Ramesh Kumar Singh\\nR/o C-45, Gomti Nagar, Lucknow – 226010\\nPhone: 9936521874\\nEmail: ritika.singh25@gmail.com\\nAadhaar: 4589 1234 7789\\n'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 4\n",
    "# Reads gold.json and rtis/ files, converts to word-level examples.\n",
    "GOLD_PATH = \"gold.json\"\n",
    "RTIS_DIR = \"rtis\"\n",
    "\n",
    "def load_gold_as_examples(gold_path=GOLD_PATH, rtis_folder=RTIS_DIR):\n",
    "    gold = json.load(open(gold_path, encoding='utf-8'))\n",
    "    examples = []\n",
    "    for fname, spans in gold.items():\n",
    "        fpath = os.path.join(rtis_folder, fname)\n",
    "        if not os.path.exists(fpath):\n",
    "            print(\"Warning: missing\", fpath)\n",
    "            continue\n",
    "        txt = open(fpath, encoding='utf-8').read()\n",
    "        # simple whitespace split into words (keeps punctuation)\n",
    "        words = txt.split()\n",
    "        # compute start/end char of each word\n",
    "        idxs = []\n",
    "        cur = 0\n",
    "        for w in words:\n",
    "            start = txt.find(w, cur)\n",
    "            if start == -1:\n",
    "                # fallback: try to move cursor forward\n",
    "                start = cur\n",
    "            idxs.append((start, start+len(w)))\n",
    "            cur = start + len(w)\n",
    "        labels = [\"O\"] * len(words)\n",
    "        for s in spans:\n",
    "            s0, s1, lab = s[\"start\"], s[\"end\"], s[\"label\"]\n",
    "            for i,(ws,we) in enumerate(idxs):\n",
    "                if not (we <= s0 or ws >= s1):\n",
    "                    labels[i] = lab\n",
    "        examples.append({\"words\": words, \"labels\": labels, \"fname\": fname, \"text\": txt})\n",
    "    return examples\n",
    "\n",
    "examples = load_gold_as_examples()\n",
    "print(f\"Loaded {len(examples)} examples\")\n",
    "examples[:1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36f9a5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'fname'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels', 'fname'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# cell 5\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_and_align_words(example):\n",
    "    tokenized = tokenizer(example[\"words\"], is_split_into_words=True, truncation=True, padding=\"max_length\", max_length=512)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    aligned_labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            aligned_labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            aligned_labels.append(label2id.get(example[\"labels\"][word_idx], 0))\n",
    "        else:\n",
    "            # subsequent tokens of the same word get -100\n",
    "            aligned_labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized[\"labels\"] = aligned_labels\n",
    "    tokenized[\"fname\"] = example[\"fname\"]\n",
    "    return tokenized\n",
    "\n",
    "# Build tokenized dataset list (small dataset: one sample -> still works)\n",
    "tokenized_list = [tokenize_and_align_words(ex) for ex in examples]\n",
    "dataset = Dataset.from_list(tokenized_list)\n",
    "# if very few examples, use same dataset for train and eval (sanity check)\n",
    "if len(dataset) > 5:\n",
    "    dset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "    dataset = DatasetDict({\"train\": dset[\"train\"], \"test\": dset[\"test\"]})\n",
    "else:\n",
    "    dataset = DatasetDict({\"train\": dataset, \"test\": dataset})\n",
    "\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4184296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric ready: seqeval\n"
     ]
    }
   ],
   "source": [
    "# cell 6\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def align_predictions(predictions, label_ids):\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    true_labels = []\n",
    "    true_preds = []\n",
    "    for p, l in zip(preds, label_ids):\n",
    "        pl = []\n",
    "        tl = []\n",
    "        for pred_i, lab_i in zip(p, l):\n",
    "            if lab_i != -100:\n",
    "                pl.append(id2label[pred_i])\n",
    "                tl.append(id2label[lab_i])\n",
    "        true_preds.append(pl)\n",
    "        true_labels.append(tl)\n",
    "    return true_preds, true_labels\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, label_ids = eval_pred\n",
    "    preds_list, labels_list = align_predictions(logits, label_ids)\n",
    "    results = metric.compute(predictions=preds_list, references=labels_list)\n",
    "    overall = {\n",
    "        \"precision\": results.get(\"overall_precision\", 0.0),\n",
    "        \"recall\": results.get(\"overall_recall\", 0.0),\n",
    "        \"f1\": results.get(\"overall_f1\", 0.0)\n",
    "    }\n",
    "    return overall\n",
    "\n",
    "print(\"Metric ready: seqeval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f84390e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.9.1+cpu cuda available: False\n",
      "Loading model: xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and moved to cpu\n"
     ]
    }
   ],
   "source": [
    "# sanity: load model variable and move to device\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"cuda available:\", torch.cuda.is_available())\n",
    "\n",
    "# load model if not present\n",
    "try:\n",
    "    model\n",
    "    print(\"model already defined\")\n",
    "except NameError:\n",
    "    print(\"Loading model:\", model_name)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(LABELS),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "# move model to device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "print(\"Model loaded and moved to\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "234ae57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer args ok. dataset sizes -> train: 3  eval: 3\n",
      "Model device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aasth\\AppData\\Local\\Temp\\ipykernel_20944\\2391809890.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 03:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.894982</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.050000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PERSON seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PHONE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: EMAIL seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: PAN seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\sequence_labeling.py:171: UserWarning: DATE seems not to be NE tag.\n",
      "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
      "c:\\Users\\aasth\\RTI-Redaction-BTP\\venv\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=2.1222705841064453, metrics={'train_runtime': 452.4232, 'train_samples_per_second': 0.007, 'train_steps_per_second': 0.002, 'total_flos': 783946967040.0, 'train_loss': 2.1222705841064453, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 7 (compat for transformers 4.57.1)\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForTokenClassification, AutoModelForTokenClassification\n",
    "\n",
    "# model was loaded earlier as `model`\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Use the argument names that match your transformers version (eval_strategy, save_strategy, logging_strategy)\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"rti_model_run\",\n",
    "    eval_strategy=\"epoch\",        # older name in this transformers version\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "print(\"Trainer args ok. dataset sizes -> train:\", len(dataset[\"train\"]), \" eval:\", len(dataset[\"test\"]))\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b36da9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to trained_rti_model\n"
     ]
    }
   ],
   "source": [
    "# cell 8\n",
    "OUTDIR = \"trained_rti_model\"\n",
    "trainer.save_model(OUTDIR)\n",
    "tokenizer.save_pretrained(OUTDIR)\n",
    "print(\"Saved model to\", OUTDIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6af51081",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preds_finetuned.json with entries for 3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample1.txt': [{'start': 0, 'end': 46, 'label': 'DATE'},\n",
       "  {'start': 50, 'end': 60, 'label': 'DATE'},\n",
       "  {'start': 67, 'end': 75, 'label': 'DATE'},\n",
       "  {'start': 84, 'end': 107, 'label': 'DATE'},\n",
       "  {'start': 111, 'end': 143, 'label': 'DATE'},\n",
       "  {'start': 148, 'end': 165, 'label': 'DATE'},\n",
       "  {'start': 173, 'end': 211, 'label': 'DATE'},\n",
       "  {'start': 217, 'end': 218, 'label': 'DATE'},\n",
       "  {'start': 225, 'end': 384, 'label': 'DATE'},\n",
       "  {'start': 389, 'end': 396, 'label': 'DATE'},\n",
       "  {'start': 399, 'end': 432, 'label': 'DATE'},\n",
       "  {'start': 434, 'end': 445, 'label': 'DATE'},\n",
       "  {'start': 446, 'end': 455, 'label': 'DATE'},\n",
       "  {'start': 459, 'end': 546, 'label': 'DATE'}],\n",
       " 'sample2.txt': [{'start': 0, 'end': 46, 'label': 'DATE'},\n",
       "  {'start': 50, 'end': 79, 'label': 'DATE'},\n",
       "  {'start': 83, 'end': 89, 'label': 'DATE'},\n",
       "  {'start': 98, 'end': 99, 'label': 'DATE'},\n",
       "  {'start': 102, 'end': 139, 'label': 'DATE'},\n",
       "  {'start': 148, 'end': 167, 'label': 'DATE'},\n",
       "  {'start': 178, 'end': 185, 'label': 'DATE'},\n",
       "  {'start': 189, 'end': 209, 'label': 'DATE'},\n",
       "  {'start': 214, 'end': 294, 'label': 'DATE'},\n",
       "  {'start': 302, 'end': 402, 'label': 'DATE'},\n",
       "  {'start': 404, 'end': 550, 'label': 'DATE'}],\n",
       " 'sample3.txt': [{'start': 0, 'end': 71, 'label': 'DATE'},\n",
       "  {'start': 76, 'end': 77, 'label': 'DATE'},\n",
       "  {'start': 86, 'end': 87, 'label': 'DATE'},\n",
       "  {'start': 88, 'end': 164, 'label': 'DATE'},\n",
       "  {'start': 167, 'end': 238, 'label': 'DATE'},\n",
       "  {'start': 242, 'end': 269, 'label': 'DATE'},\n",
       "  {'start': 282, 'end': 381, 'label': 'DATE'},\n",
       "  {'start': 388, 'end': 414, 'label': 'DATE'},\n",
       "  {'start': 417, 'end': 460, 'label': 'DATE'},\n",
       "  {'start': 462, 'end': 517, 'label': 'DATE'},\n",
       "  {'start': 522, 'end': 535, 'label': 'DATE'}]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 9\n",
    "# Build a token-classification pipeline using the saved model\n",
    "pipe = pipeline(\"token-classification\", model=OUTDIR, tokenizer=OUTDIR, aggregation_strategy=\"simple\")\n",
    "\n",
    "rtis_files = [ex[\"fname\"] for ex in examples]  # only the examples you loaded\n",
    "all_preds = {}\n",
    "\n",
    "for fname in rtis_files:\n",
    "    text = open(os.path.join(RTIS_DIR, fname), encoding='utf-8').read()\n",
    "    preds = pipe(text)\n",
    "    spans = []\n",
    "    # convert pipeline outputs to {start,end,label}\n",
    "    for p in preds:\n",
    "        # p has keys: entity_group (label), score, word, start, end\n",
    "        lbl = p.get(\"entity_group\", p.get(\"entity\", None))\n",
    "        if lbl is None:\n",
    "            continue\n",
    "        spans.append({\"start\": int(p[\"start\"]), \"end\": int(p[\"end\"]), \"label\": str(lbl)})\n",
    "    all_preds[fname] = spans\n",
    "\n",
    "# Save to preds_finetuned.json\n",
    "with open(\"preds_finetuned.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_preds, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Wrote preds_finetuned.json with entries for\", len(all_preds))\n",
    "all_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9fbdf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline preds.json (if exists) and finetuned preds_finetuned.json\n",
      "Baseline:\n",
      "\n",
      "Fine-tuned:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['c:\\\\Users\\\\aasth\\\\RTI-Redaction-BTP\\\\venv\\\\Scripts\\\\python.exe', 'eval_script.py', 'gold.json', 'preds_finetuned.json'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell 10 (optional)\n",
    "import subprocess, sys\n",
    "print(\"Evaluating baseline preds.json (if exists) and finetuned preds_finetuned.json\")\n",
    "if os.path.exists(\"preds.json\"):\n",
    "    print(\"Baseline:\")\n",
    "    subprocess.run([sys.executable, \"eval_script.py\", \"gold.json\", \"preds.json\"])\n",
    "print(\"\\nFine-tuned:\")\n",
    "subprocess.run([sys.executable, \"eval_script.py\", \"gold.json\", \"preds_finetuned.json\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
